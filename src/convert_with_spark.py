from dotenv import load_dotenv
import os
import boto3
import glob
import pandas as pd
from pyspark.sql import SparkSession
import socket
import shutil


# Cargar variables del entorno
load_dotenv()

CSV_DIR = '/home/mrgonzalez/Desktop/PYTHON/CARKICK/data/csv'
PARQUET_DIR = '/home/mrgonzalez/Desktop/PYTHON/CARKICK/data/parquet'
BUCKET_NAME = 'datasets'

os.makedirs(PARQUET_DIR, exist_ok=True)

def test_spark_connection(spark_master_url):
    """Verifica si el Spark Master est√° accesible"""
    try:
        host = spark_master_url.replace('spark://', '').split(':')[0]
        port = int(spark_master_url.replace('spark://', '').split(':')[1])
        
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(5)
        result = sock.connect_ex((host, port))
        sock.close()
        
        return result == 0
    except Exception as e:
        print(f"‚ùå Error probando conexi√≥n: {e}")
        return False

# Inicializar Spark
spark = None
try:
    # Opciones de conexi√≥n a Spark Master (prioridad de m√°s probable a menos probable)
    spark_master_options = [
        "local[*]",                   # Modo local primero (para debugging)
        "spark://localhost:7077",     # Si ejecutas desde host
        "spark://spark-master:7077",  # Usando nombre del contenedor
        "spark://172.20.0.3:7077"     # Tu IP original
    ]
    
    spark_master_url = None
    
    for master_url in spark_master_options:
        print(f"üîç Probando conexi√≥n a: {master_url}")
        
        if master_url == "local[*]":
            print("‚ö†Ô∏è Usando modo local de Spark (sin cluster)")
            spark_master_url = master_url
            break
        elif test_spark_connection(master_url):
            print(f"‚úÖ Conexi√≥n exitosa a: {master_url}")
            spark_master_url = master_url
            break
        else:
            print(f"‚ùå No se pudo conectar a: {master_url}")
    
    if not spark_master_url:
        raise RuntimeError("‚ùå No se pudo conectar a ning√∫n Spark Master")
    
    print(f"üöÄ Inicializando Spark con: {spark_master_url}")
    
    spark = SparkSession.builder \
        .appName("convert_with_spark.py") \
        .master(spark_master_url) \
        .config("spark.executor.instances", "1") \
        .config("spark.executor.cores", "2") \
        .config("spark.executor.memory", "1g") \
        .config("spark.cores.max", "4") \
        .config("spark.sql.adaptive.enabled", "false") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .getOrCreate()

    # Validar que Spark est√° funcionando
    print("üß™ Probando funcionalidad de Spark...")
    test_df = spark.range(1, 5)
    test_count = test_df.count()
    print(f"‚úÖ Spark funcionando correctamente. Test count: {test_count}")

    # Verificar si hay archivos CSV
    if not os.path.exists(CSV_DIR):
        raise FileNotFoundError(f"‚ùå Directorio CSV no existe: {CSV_DIR}")
    
    csv_files = [f for f in os.listdir(CSV_DIR) if f.endswith('.csv')]
    if not csv_files:
        print(f"‚ö†Ô∏è No se encontraron archivos CSV en: {CSV_DIR}")
        exit(0)
    
    print(f"üìÅ Encontrados {len(csv_files)} archivos CSV: {csv_files}")

    # Inicializar cliente MinIO/S3
    try:
        print("üîå Conectando a MinIO...")
        
        # Obtener credenciales del archivo .env
        aws_access_key = os.getenv('AWS_ACCESS_KEY_ID')
        aws_secret_key = os.getenv('AWS_SECRET_ACCESS_KEY')
        
        if not aws_access_key or not aws_secret_key:
            raise ValueError("‚ùå Credenciales AWS no encontradas en .env")
        
        print(f"üîë Usando credenciales: {aws_access_key}")
        
        s3 = boto3.client(
            's3', 
            endpoint_url='http://localhost:9100',
            aws_access_key_id=aws_access_key,
            aws_secret_access_key=aws_secret_key
        )
        
        # Verificar conexi√≥n a MinIO
        s3.head_bucket(Bucket=BUCKET_NAME)
        print("‚úÖ Conexi√≥n a MinIO exitosa")
        
    except Exception as e:
        print(f"‚ùå Error conectando a MinIO: {e}")
        print("üí° Aseg√∫rate de que MinIO est√© ejecut√°ndose y el bucket existe")
        raise

    # Procesar CSVs
    for filename in csv_files:
        try:
            csv_path = os.path.join(CSV_DIR, filename)
            parquet_filename = filename.replace('.csv', '.parquet')
            #output_path = os.path.join(PARQUET_DIR, parquet_filename)
            base_name = os.path.splitext(filename)[0]
            output_path = os.path.join(PARQUET_DIR, base_name)

            print(f"\nüì¶ Procesando: {filename}")
            print(f"   üìç Origen: {csv_path}")
            print(f"   üìç Destino: {output_path}")

            # Leer CSV con pandas primero para validar
            print("   üîç Validating CSV...")
            pdf = pd.read_csv(csv_path)
            print(f"   üìä Filas: {len(pdf)}, Columnas: {len(pdf.columns)}")
            
            if pdf.empty:
                print(f"   ‚ö†Ô∏è Archivo CSV vac√≠o, saltando: {filename}")
                continue

            # Crear DataFrame de Spark
            print("   üîÑ Convirtiendo a Spark DataFrame...")
            df = spark.createDataFrame(pdf)

            

            # Eliminar el archivo o directorio de salida si ya existe
            if os.path.exists(output_path):
                print(f"üßπ Limpiando destino anterior: {output_path}")
                try:
                    if os.path.isfile(output_path):
                        os.remove(output_path)
                    else:
                        shutil.rmtree(output_path)
                except Exception as e:
                    print(f"‚ö†Ô∏è Error eliminando destino previo: {e}")
                    raise

            # Escribir Parquet
            print(f"   üíæ Escribiendo Parquet...")
            df.coalesce(1).write.mode("overwrite").parquet(output_path)

            # Buscar archivo part-*.parquet
            part_files = glob.glob(os.path.join(output_path, 'part-*.parquet'))
            if part_files:
                part_file = part_files[0]
                print(f"   ‚òÅÔ∏è Subiendo a MinIO: {parquet_filename}")
                
                with open(part_file, 'rb') as data:
                    s3.upload_fileobj(data, BUCKET_NAME, parquet_filename)
                
                print(f"   ‚úÖ {filename} procesado exitosamente")
            else:
                print(f"   ‚ùå No se encontr√≥ archivo parquet generado para {filename}")

        except Exception as file_error:
            print(f"   üö® Error procesando {filename}: {file_error}")
            continue

    print("\nüéâ Conversi√≥n y subida completadas con Spark.")

except Exception as e:
    print(f"üö® Error durante la ejecuci√≥n: {e}")
    print("\nüîß Sugerencias de debugging:")
    print("1. Verifica que los contenedores de Spark est√©n ejecut√°ndose:")
    print("   docker ps | grep spark")
    print("2. Verifica que MinIO est√© ejecut√°ndose:")
    print("   docker ps | grep minio")
    print("3. Verifica que el directorio CSV existe y tiene archivos:")
    print(f"   ls -la {CSV_DIR}")
    print("4. Si ejecutas desde un contenedor, usa 'spark-master:7077'")
    print("5. Si ejecutas desde el host, usa 'localhost:7077'")

finally:
    if spark:
        try:
            spark.stop()
            print("üõë Spark detenido correctamente.")
        except Exception as stop_error:
            print(f"‚ö†Ô∏è Error deteniendo Spark: {stop_error}")