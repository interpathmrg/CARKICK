
"""
prepare_from_minio_direct.py
Convierte training.parquet (Bronze) ‚ûú training.silver.parquet usando Spark con acceso directo a MinIO
Lee y escribe directamente desde/hacia MinIO sin descargas locales
"""

from dotenv import load_dotenv
load_dotenv()

import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, upper, when, trim
from pyspark.ml.feature import StringIndexer
from pyspark.ml import Pipeline

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ CONFIG ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
BRONZE_FILE = "training.parquet"
SILVER_FILE = "training.silver.parquet"
BUCKET = "datasets"
# MINIO_ENDPOINT = "localhost:9100"  # Sin http:// para Spark - This was potentially confusing
# MINIO_URL = "http://localhost:9100"  # Con http:// para display - This was potentially confusing

# Correct MinIO endpoint for Spark and display (assuming MinIO runs on 9000 for S3 API)
MINIO_SPARK_ENDPOINT = "http://localhost:9100"
MINIO_DISPLAY_URL = "http://localhost:9100" # For consistency in display messages

ACCESS_KEY = os.getenv("AWS_ACCESS_KEY_ID")
SECRET_KEY = os.getenv("AWS_SECRET_ACCESS_KEY")

print("üîê AWS_ACCESS_KEY_ID:", ACCESS_KEY) # Good to print the actual value being used
print("üîê AWS_SECRET_ACCESS_KEY:", "********" if SECRET_KEY else None) # Mask secret key

def create_spark_session():
    """Crea una sesi√≥n de Spark configurada para MinIO"""
    # Choose versions compatible with your Spark/Hadoop setup
    # For Spark 3.3.x, Hadoop 3.3.x is common
    # Check https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-aws
    # and https://mvnrepository.com/artifact/com.amazonaws/aws-java-sdk-bundle
    hadoop_aws_version = "3.3.4"  # Example, adjust if needed
    aws_sdk_version = "1.12.367" # Example, adjust if needed

    return (
        SparkSession.builder
            .appName("MinIO Bronze to Silver") # More descriptive app name
            # .appName("App") # Redundant, removed
            .config("spark.hadoop.fs.s3a.access.key", ACCESS_KEY)
            .config("spark.hadoop.fs.s3a.secret.key", SECRET_KEY)
            .config("spark.hadoop.fs.s3a.endpoint", MINIO_SPARK_ENDPOINT) # Use the defined constant
            .config("spark.hadoop.fs.s3a.path.style.access", "true")
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
            # --- ADD THESE LINES ---
            .config("spark.jars.packages", 
                    f"org.apache.hadoop:hadoop-aws:{hadoop_aws_version},"
                    f"com.amazonaws:aws-java-sdk-bundle:{aws_sdk_version}")
            # For local development, ensure Spark can run
            # .master("local[*]") # Uncomment if not running through spark-submit with a master defined
            .getOrCreate()
    )

def main():
    print("üöÄ Iniciando procesamiento Bronze ‚Üí Silver con acceso directo a MinIO")

    # Crear sesi√≥n Spark
    spark = create_spark_session()

    try:
        # Rutas S3A para MinIO
        bronze_path = f"s3a://{BUCKET}/{BRONZE_FILE}"
        silver_path = f"s3a://{BUCKET}/{SILVER_FILE}"

        print(f"üìñ Leyendo archivo Bronze desde: {bronze_path}")
        print(f"‚ÑπÔ∏è Usando MinIO endpoint para Spark: {MINIO_SPARK_ENDPOINT}")

        # 1Ô∏è‚É£ Leer datos Bronze directamente desde MinIO
        df = spark.read.parquet(bronze_path)

        print(f"‚úÖ Datos le√≠dos exitosamente. Registros: {df.count():,}")
        print("üìä Esquema del dataset:")
        df.printSchema()

        # 2Ô∏è‚É£ Selecci√≥n y limpieza de datos
        print("üßπ Aplicando transformaciones y limpieza...")

        # ... (rest of your data transformation logic remains the same) ...
        df_cleaned = (
            df.select(
                "IsBadBuy", "VehYear", "VehicleAge", "VehOdo", "VehBCost",
                "IsOnlineSale", "Transmission", "Size",
                "MMRAcquisitionRetailAveragePrice",
                "MMRCurrentAuctionAveragePrice", "MMRCurrentAuctionCleanPrice",
            )
            .withColumn(
                "Transmission_clean",
                when(upper(trim(col("Transmission"))) == "MANUAL", "MANUAL")
                .when(upper(trim(col("Transmission"))) == "AUTO", "AUTO")
                .otherwise(None), # Consider using a specific "UNKNOWN" string if None causes issues downstream
            )
            .withColumn(
                "Size_clean",
                when(
                    trim(upper(col("Size"))).isin( # Standardize to upper for comparison
                        "SPORTS", "SMALL SUV", "CROSSOVER", "VAN", "COMPACT",
                        "SMALL TRUCK", "SPECIALTY", "MEDIUM", "MEDIUM SUV",
                        "LARGE SUV", "LARGE TRUCK", "LARGE"
                    ),
                    trim(upper(col("Size"))), # Store the standardized upper case version
                ).otherwise(None), # Consider "UNKNOWN"
            )
        )

        # 3Ô∏è‚É£ Aplicar StringIndexer para variables categ√≥ricas
        print("üî¢ Aplicando StringIndexer...")

        indexers = [
            StringIndexer(
                inputCol="Transmission_clean",
                outputCol="Transmission_idx",
                handleInvalid="keep", # 'keep' will assign a special index to unseen/null values
            ),
            StringIndexer(
                inputCol="Size_clean",
                outputCol="Size_idx",
                handleInvalid="keep",
            ),
        ]

        pipeline = Pipeline(stages=indexers)
        pipeline_model = pipeline.fit(df_cleaned)
        df_indexed = pipeline_model.transform(df_cleaned)

        # 4Ô∏è‚É£ Selecci√≥n final de columnas Silver
        silver_columns = [
            "IsBadBuy", "VehYear", "VehicleAge", "VehOdo", "VehBCost",
            "IsOnlineSale", "Transmission_idx", "Size_idx",
            "MMRAcquisitionRetailAveragePrice",
            "MMRCurrentAuctionAveragePrice", "MMRCurrentAuctionCleanPrice",
        ]

        silver_df = df_indexed.select(silver_columns)

        print("üìà Dataset Silver preparado:")
        print(f"   ‚Ä¢ Registros: {silver_df.count():,}")
        print(f"   ‚Ä¢ Columnas: {len(silver_df.columns)}")

        print("\nüìä Primeras 5 filas del dataset Silver:")
        silver_df.show(5)

        # 5Ô∏è‚É£ Guardar directamente en MinIO
        print(f"üíæ Guardando archivo Silver en: {silver_path}")

        (
            silver_df
            .repartition(1)
            .write
            .mode("overwrite")
            .option("compression", "snappy")
            .parquet(silver_path)
        )

        print("‚úÖ Archivo Silver guardado exitosamente en MinIO!")
        # Use the consistent display URL
        print(f"üîó Ubicaci√≥n: {MINIO_DISPLAY_URL}/{BUCKET}/{SILVER_FILE}")

        print("\nüîç Verificando archivo guardado...")
        verification_df = spark.read.parquet(silver_path)
        print(f"   ‚úì Registros verificados: {verification_df.count():,}")

    except Exception as e:
        print(f"‚ùå Error durante el procesamiento: {str(e)}")
        import traceback
        traceback.print_exc() # Print full traceback for better debugging
        # raise # Re-raise if you want the script to exit with an error code

    finally:
        print("\nüßπ Cerrando sesi√≥n Spark...")
        if 'spark' in locals() and spark: # Check if spark session was created
            spark.stop()
        print("üõë Procesamiento completado.")

if __name__ == "__main__":
    main()