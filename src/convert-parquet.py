from dotenv import load_dotenv
import os
# import boto3 # boto3 might still be useful for pre-checks like bucket existence, but not for data writing via Spark
import glob
import pandas as pd
from pyspark.sql import SparkSession
import socket
# import shutil # No longer needed for local parquet cleanup if writing directly to S3A

# Cargar variables del entorno
load_dotenv()

HOST_CSV_DIR = '/home/mrgonzalez/Desktop/PYTHON/CARKICK/data/csv'
CONTAINER_CSV_DIR_FOR_SPARK = '/data/csv'

BUCKET_NAME = 'datasets' # MinIO Bucket Name

# S3A Configuration (ensure these are correct for your MinIO setup)
MINIO_SPARK_ENDPOINT = "http://172.17.0.1:9100" # S3A endpoint for Spark
ACCESS_KEY       = os.getenv("AWS_ACCESS_KEY_ID")
SECRET_KEY       = os.getenv("AWS_SECRET_ACCESS_KEY")

if not ACCESS_KEY or not SECRET_KEY:
    raise EnvironmentError("‚ùå Falta AWS_ACCESS_KEY_ID / SECRET_KEY en .env")

# No need to create local PARQUET_DIR if writing directly to S3A
# os.makedirs(PARQUET_DIR, exist_ok=True)

def test_spark_connection(spark_master_url):
    """Verifica si el Spark Master est√° accesible"""
    try:
        host = spark_master_url.replace('spark://', '').split(':')[0]
        port = int(spark_master_url.replace('spark://', '').split(':')[1])
        
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(5)
        result = sock.connect_ex((host, port))
        sock.close()
        return result == 0
    except Exception as e:
        print(f"‚ùå Error probando conexi√≥n a Spark Master '{spark_master_url}': {e}")
        return False

# <<<--- CONFIGURACI√ìN CR√çTICA ---<<<
driver_announce_ip = "172.17.0.1" # La IP de docker0 que los contenedores S√ç PUEDEN VER
print(f"üì¢ Anunciando IP del driver para los ejecutores: {driver_announce_ip}")

spark = None
try:
    spark_master_options = [
    "spark://172.17.0.1:7077",     # Si ejecutas desde host y est√° mapeado
    "spark://localhost:7077",     # Si ejecutas desde host y est√° mapeado
    "spark://spark-master:7077",  # Si el script se ejecuta DENTRO de un contenedor en la misma red Docker
    # "spark://<IP_DEL_HOST>:7077", # Otra opci√≥n si localhost no funciona desde el script
    "local[*]"                    # Modo local como √∫ltimo recurso
    ]

    spark_master_url = None
    for master_url_option in spark_master_options:
        print(f"üîç Probando conexi√≥n a Spark Master: {master_url_option}")
        if master_url_option == "local[*]":
            print("‚ö†Ô∏è Usando modo local de Spark (sin cluster).")
            spark_master_url = master_url_option
            break
        elif test_spark_connection(master_url_option):
            print(f"‚úÖ Conexi√≥n exitosa a Spark Master: {master_url_option}")
            spark_master_url = master_url_option
            break
        else:
            print(f"‚ùå No se pudo conectar a Spark Master: {master_url_option}")
    
    if not spark_master_url:
        raise RuntimeError("‚ùå No se pudo conectar a ning√∫n Spark Master de las opciones probadas.")
    
    print(f"üöÄ Inicializando Spark con Master: {spark_master_url}")

    # ... (cerca de donde defines hadoop_aws_version y aws_sdk_version) ...
    hadoop_aws_version = "3.3.4"
    aws_sdk_version = "1.12.367"
    
    spark_builder = SparkSession.builder \
        .appName("CSVtoParquetDirectToMinIO") \
        .master(spark_master_url) \
        .config("spark.driver.host", driver_announce_ip) \
            .config("spark.hadoop.fs.s3a.access.key", ACCESS_KEY) \
            .config("spark.hadoop.fs.s3a.secret.key", SECRET_KEY)\
            .config("spark.hadoop.fs.s3a.endpoint", MINIO_SPARK_ENDPOINT)\
            .config("spark.hadoop.fs.s3a.path.style.access", "true")\
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")\
            .config("spark.jars.packages",
                    f"org.apache.hadoop:hadoop-aws:{hadoop_aws_version},"\
                    f"com.amazonaws:aws-java-sdk-bundle:{aws_sdk_version}")\
    
    spark = spark_builder.getOrCreate()

    print("üß™ Probando funcionalidad de Spark...")
    test_df = spark.range(1, 5)
    test_count = test_df.count()
    print(f"‚úÖ Spark funcionando correctamente. Test count: {test_count}")

    if not os.path.exists(HOST_CSV_DIR):
        raise FileNotFoundError(f"‚ùå Directorio CSV no existe: {HOST_CSV_DIR}")
    
    csv_files = [f for f in os.listdir(HOST_CSV_DIR) if f.endswith('.csv')]

    if not csv_files:
        print(f"‚ö†Ô∏è No se encontraron archivos CSV en: {HOST_CSV_DIR}")
        exit(0)
    
    print(f"üìÅ Encontrados {len(csv_files)} archivos CSV: {csv_files}")

    # (Optional) Pre-check MinIO bucket existence with boto3 if desired
    # import boto3
    # try:
    #     s3_check_client = boto3.client(
    #         's3',
    #         endpoint_url=MINIO_SPARK_ENDPOINT, # Use the same endpoint
    #         aws_access_key_id=ACCESS_KEY,
    #         aws_secret_access_key=SECRET_KEY
    #     )
    #     s3_check_client.head_bucket(Bucket=BUCKET_NAME)
    #     print(f"‚úÖ Bucket MinIO '{BUCKET_NAME}' verificado.")
    # except Exception as e_boto:
    #     print(f"‚ùå Error verificando bucket MinIO '{BUCKET_NAME}': {e_boto}")
    #     print("üí° Aseg√∫rate de que MinIO est√© ejecut√°ndose y el bucket exista antes de que Spark intente escribir.")
    #     # raise # You might want to raise an error here if bucket check is critical

    for filename in csv_files:
        try:
            spark_worker_csv_path = os.path.join(CONTAINER_CSV_DIR_FOR_SPARK, filename)
            # Define the S3A path for the output Parquet "directory"
            # Spark will create a directory named parquet_filename in the bucket
            parquet_filename_on_s3 = filename.replace('.csv', '.parquet')
            s3a_output_path = f"s3a://{BUCKET_NAME}/{parquet_filename_on_s3}"

            print(f"\nüì¶ Procesando: {filename}")
            print(f"   üìç Origen (local CSV): {spark_worker_csv_path }")
            print(f"   üìç Destino (MinIO S3A): {s3a_output_path}")

            print("   üîç Validando CSV con Pandas...")
            # Dejar que Spark lea el CSV directamente
            print("   üîÑ Leyendo CSV directamente con Spark...")
            df = spark.read.option("header", "true") \
               .option("inferSchema", "true") \
               .csv(spark_worker_csv_path) # Spark lee desde la ruta del worker
            
            if df.rdd.isEmpty():
                print(f"   ‚ö†Ô∏è DataFrame de Spark vac√≠o despu√©s de leer, saltando: {filename}")
                continue

            # Esta cuenta ahora s√≠ se ejecutar√° distribuida
            print(f"   üìä Filas le√≠das por Spark: {df.count()}, Columnas: {len(df.columns)}")
            
            # ... justo antes de escribir ...
            count_before_write = df.count()
            print(f"   üìè Verificaci√≥n ANTES de escribir: DataFrame tiene {count_before_write} filas.")
            if count_before_write == 0:
                print(f"   üõë ALERTA: DataFrame para {filename} est√° vac√≠o. No se escribir√°n datos Parquet.")
                # Decide si quieres continuar y crear un directorio vac√≠o con _SUCCESS o detenerte/manejarlo.
                # Por ahora, para debugging, podr√≠as dejar que contin√∫e y cree el _SUCCESS
                # o podr√≠as hacer un 'continue' para saltar la escritura.

            (df.write
            .mode("overwrite")
            .parquet(s3a_output_path)
)
            # The output at s3a_output_path is a directory containing _SUCCESS and part-*.parquet files.
            # If you need a single Parquet file named exactly 'file.parquet' directly in the bucket
            # (not a directory), you'd need the post-processing step with MinIO client as in claude-prepare_from_minio.py
            # For now, this writes a directory, which is standard Spark behavior.

            print(f"   ‚úÖ {filename} procesado y escrito a MinIO como Parquet en el directorio: {parquet_filename_on_s3}")

        except Exception as file_error:
            import traceback
            print(f"   üö® Error procesando {filename}: {file_error}")
            traceback.print_exc() # Print full traceback for file errors
            continue

    print("\nüéâ Conversi√≥n y escritura directa a MinIO completadas con Spark.")

except Exception as e:
    import traceback
    print(f"üö® Error general durante la ejecuci√≥n: {e}")
    traceback.print_exc() # Print full traceback for general errors
    print("\nüîß Sugerencias de debugging:")
    print("1. Verifica que los contenedores de Spark (master/worker) est√©n ejecut√°ndose.")
    print("2. Verifica que MinIO est√© ejecut√°ndose y accesible en el endpoint configurado.")
    print(f"3. Verifica que el bucket '{BUCKET_NAME}' exista en MinIO.")
    print(f"4. Verifica que el directorio CSV '{HOST_CSV_DIR}' existe y tiene archivos.")
    print("5. Revisa las credenciales de MinIO en tu archivo .env.")
    print("6. Aseg√∫rate de que las versiones de hadoop-aws y aws-java-sdk-bundle sean compatibles con tu Spark.")


finally:
    if spark:
        try:
            spark.stop()
            print("üõë Spark detenido correctamente.")
        except Exception as stop_error:
            print(f"‚ö†Ô∏è Error deteniendo Spark: {stop_error}")